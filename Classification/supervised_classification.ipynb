{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification supervisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milestone 4 : Faisabilité de classification automatique d’images via CNN Transfer Learning\n",
    "\n",
    "\n",
    "Livrable :\n",
    "Notebook d’analyse de faisabilité de classification automatique d’images via CNN Transfer Learning.\n",
    "Problèmes et erreurs courants :\n",
    "L’objectif est de vérifier la faisabilité de classifier automatiquement les images, simplement via une représentation en 2D des images et une vérification d’une séparation automatique selon leur catégorie réelle. Une classification supervisée de prédiction de catégories des images n’est demandée que dans un 2ème temps (CF milestone 5).\n",
    "Recommandations :\n",
    "Récupérer un modèle pré-entraîné comme précisé dans la ressource « Transfer Learning in Keras with Computer Vision Models », en particulier le paragraphe « Pre-Trained Model as Feature Extractor Preprocessor ».\n",
    "La suite est identique au milestone 3 (SIFT) :\n",
    "ACP, T-SNE, k-means, affichage des 2 composantes T-SNE des images coloriées selon la catégorie réelle, puis selon le numéro de cluster, calcul ARI.\n",
    "Le résultat tant visuel que calculé (0.4 à 0.6) est bien plus pertinent et montre, sans entraînement d’un modèle, la faisabilité de réaliser une classification automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category_encoded</th>\n",
       "      <th>main_category</th>\n",
       "      <th>image</th>\n",
       "      <th>511</th>\n",
       "      <th>510</th>\n",
       "      <th>509</th>\n",
       "      <th>508</th>\n",
       "      <th>507</th>\n",
       "      <th>506</th>\n",
       "      <th>505</th>\n",
       "      <th>...</th>\n",
       "      <th>9</th>\n",
       "      <th>8</th>\n",
       "      <th>7</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Kitchen &amp; Dining</td>\n",
       "      <td>aa68675f50a0551b8dadb954017a50a1.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.073370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>21.298357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.981668</td>\n",
       "      <td>3.415755</td>\n",
       "      <td>0.887089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.663045</td>\n",
       "      <td>0.618434</td>\n",
       "      <td>0.698465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.633072</td>\n",
       "      <td>0.249923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>037c2402fee39fbc80433935711d1383.jpg</td>\n",
       "      <td>1.093953</td>\n",
       "      <td>0.055345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.936242</td>\n",
       "      <td>20.157139</td>\n",
       "      <td>6.217503</td>\n",
       "      <td>...</td>\n",
       "      <td>4.976550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421278</td>\n",
       "      <td>6.777927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.038126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>42643c1c9403f67921a18654bcf45ead.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.847941</td>\n",
       "      <td>3.689030</td>\n",
       "      <td>3.193869</td>\n",
       "      <td>2.963480</td>\n",
       "      <td>0.120899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.589621</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0.028752</td>\n",
       "      <td>0.676027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Home Decor &amp; Festive Needs</td>\n",
       "      <td>53f4bc7d7321f5c41de6b86e41f13e80.jpg</td>\n",
       "      <td>0.038787</td>\n",
       "      <td>2.439613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.974669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.438453</td>\n",
       "      <td>1.075916</td>\n",
       "      <td>...</td>\n",
       "      <td>20.681154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.303908</td>\n",
       "      <td>1.767682</td>\n",
       "      <td>10.229548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.692279</td>\n",
       "      <td>21.113321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Watches</td>\n",
       "      <td>b144a363c107c7bdd91f32d6e28ba6f2.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.817364</td>\n",
       "      <td>0.861340</td>\n",
       "      <td>20.981436</td>\n",
       "      <td>13.588867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.586900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.979332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.521687</td>\n",
       "      <td>1.674501</td>\n",
       "      <td>0.168106</td>\n",
       "      <td>0.463006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 515 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   main_category_encoded               main_category  \\\n",
       "0                      5            Kitchen & Dining   \n",
       "1                      4             Home Furnishing   \n",
       "2                      0                   Baby Care   \n",
       "3                      3  Home Decor & Festive Needs   \n",
       "4                      6                     Watches   \n",
       "\n",
       "                                  image       511       510       509  \\\n",
       "0  aa68675f50a0551b8dadb954017a50a1.jpg  0.000000  4.073370  0.000000   \n",
       "1  037c2402fee39fbc80433935711d1383.jpg  1.093953  0.055345  0.000000   \n",
       "2  42643c1c9403f67921a18654bcf45ead.jpg  0.000000  0.000000  1.847941   \n",
       "3  53f4bc7d7321f5c41de6b86e41f13e80.jpg  0.038787  2.439613  0.000000   \n",
       "4  b144a363c107c7bdd91f32d6e28ba6f2.jpg  0.000000  1.817364  0.861340   \n",
       "\n",
       "         508        507        506       505  ...          9         8  \\\n",
       "0   0.005690  21.298357   0.000000  0.262152  ...   1.981668  3.415755   \n",
       "1   0.000000   6.936242  20.157139  6.217503  ...   4.976550  0.000000   \n",
       "2   3.689030   3.193869   2.963480  0.120899  ...   0.000000  0.000000   \n",
       "3   2.974669   0.000000  11.438453  1.075916  ...  20.681154  0.000000   \n",
       "4  20.981436  13.588867   0.000000  1.586900  ...   0.155157  0.000000   \n",
       "\n",
       "          7         6         5         4          3         2         1  \\\n",
       "0  0.887089  0.000000  2.663045  0.618434   0.698465  0.000000  3.633072   \n",
       "1  0.421278  6.777927  0.000000  2.038126   0.000000  0.000000  0.455629   \n",
       "2  2.589621  0.879173  0.028752  0.676027   0.000000  0.880566  0.000000   \n",
       "3  0.000000  0.000000  5.303908  1.767682  10.229548  0.000000  2.692279   \n",
       "4  0.000000  2.979332  0.000000  1.521687   1.674501  0.168106  0.463006   \n",
       "\n",
       "           0  \n",
       "0   0.249923  \n",
       "1   0.000000  \n",
       "2   5.008817  \n",
       "3  21.113321  \n",
       "4   0.000000  \n",
       "\n",
       "[5 rows x 515 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images = pd.read_csv('features_images_df.csv')\n",
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemin dossier images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossier_images = \"../Source/Images/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG sans data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 306s 11s/step - loss: 2.2288 - accuracy: 0.1500 - val_loss: 2.5645 - val_accuracy: 0.1524\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 307s 11s/step - loss: 1.9844 - accuracy: 0.1321 - val_loss: 1.9470 - val_accuracy: 0.1333\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 312s 12s/step - loss: 1.9493 - accuracy: 0.1345 - val_loss: 1.9471 - val_accuracy: 0.1333\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 300s 11s/step - loss: 1.9461 - accuracy: 0.1512 - val_loss: 1.9473 - val_accuracy: 0.1333\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 298s 11s/step - loss: 1.9459 - accuracy: 0.1512 - val_loss: 1.9480 - val_accuracy: 0.1333\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 299s 11s/step - loss: 1.9458 - accuracy: 0.1512 - val_loss: 1.9480 - val_accuracy: 0.1333\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 299s 11s/step - loss: 1.9458 - accuracy: 0.1512 - val_loss: 1.9476 - val_accuracy: 0.1333\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 299s 11s/step - loss: 1.9457 - accuracy: 0.1512 - val_loss: 1.9476 - val_accuracy: 0.1333\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 310s 11s/step - loss: 1.9457 - accuracy: 0.1512 - val_loss: 1.9484 - val_accuracy: 0.1333\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 286s 11s/step - loss: 1.9456 - accuracy: 0.1512 - val_loss: 1.9483 - val_accuracy: 0.1333\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.9493 - accuracy: 0.0857\n",
      "Loss: 1.9492673873901367, Accuracy: 0.08571428805589676\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement des images\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype('float32') / 255.0  # Normalisation\n",
    "    return img\n",
    "\n",
    "# Charger les images et les catégories\n",
    "images = []\n",
    "labels = []\n",
    "for idx, row in df_images.iterrows():\n",
    "    image_path = os.path.join('../Source/Images/', row['image'])\n",
    "    image = preprocess_image(image_path)\n",
    "    images.append(image)\n",
    "    labels.append(row['main_category_encoded'])\n",
    "\n",
    "# Convertir les listes en tableaux numpy\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Charger le modèle pré-entraîné\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Créer le modèle de classification\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # 7 classes pour la classification\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mesurer le temps d'entraînement\n",
    "start_time = time.time()\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Mesurer le temps écoulé\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Évaluer le modèle\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "print(\"Temps d'entraînement VGG sans data augmentation :\", training_time, \"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarek/anaconda3/lib/python3.10/site-packages/PIL/Image.py:3167: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 110s 4s/step - loss: 2.1702 - accuracy: 0.6595 - val_loss: 1.3996 - val_accuracy: 0.8143\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 105s 4s/step - loss: 0.6442 - accuracy: 0.8524 - val_loss: 1.1996 - val_accuracy: 0.8143\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 100s 4s/step - loss: 0.3069 - accuracy: 0.9119 - val_loss: 1.0443 - val_accuracy: 0.8095\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 108s 4s/step - loss: 0.2008 - accuracy: 0.9369 - val_loss: 1.1213 - val_accuracy: 0.8429\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 107s 4s/step - loss: 0.1452 - accuracy: 0.9464 - val_loss: 1.0410 - val_accuracy: 0.8381\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 99s 4s/step - loss: 0.1192 - accuracy: 0.9524 - val_loss: 1.0122 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 100s 4s/step - loss: 0.1312 - accuracy: 0.9548 - val_loss: 1.0979 - val_accuracy: 0.8429\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 100s 4s/step - loss: 0.0704 - accuracy: 0.9786 - val_loss: 1.0912 - val_accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 100s 4s/step - loss: 0.0791 - accuracy: 0.9810 - val_loss: 1.1786 - val_accuracy: 0.8429\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 98s 4s/step - loss: 0.0469 - accuracy: 0.9821 - val_loss: 1.1451 - val_accuracy: 0.8524\n",
      "7/7 [==============================] - 19s 3s/step - loss: 1.1451 - accuracy: 0.8524\n",
      "Loss: 1.1451117992401123\n",
      "Accuracy: 0.8523809313774109\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Charger le modèle VGG16 pré-entraîné\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Ajouter des couches personnalisées au modèle VGG16\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(df_images['main_category_encoded'].unique()), activation='softmax')(x)\n",
    "\n",
    "# Définir le nouveau modèle\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Geler les couches du modèle VGG16\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Charger et prétraiter les images\n",
    "def charger_et_pretraiter_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# Extraire les features pour toutes les images\n",
    "features_list = []\n",
    "for image_name in df_images['image']:\n",
    "    chemin_image = os.path.join(dossier_images, image_name)\n",
    "    features = charger_et_pretraiter_image(chemin_image)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Convertir la liste de features en tableau numpy\n",
    "features_array = np.vstack(features_list)\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, \n",
    "                                                    to_categorical(df_images['main_category_encoded']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=df_images['main_category_encoded'])\n",
    "\n",
    "# Créer un générateur d'augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Rotation aléatoire des images dans une plage de 20 degrés\n",
    "    width_shift_range=0.2,  # Déplacement horizontal aléatoire des images dans une plage de 20%\n",
    "    height_shift_range=0.2,  # Déplacement vertical aléatoire des images dans une plage de 20%\n",
    "    shear_range=0.2,  # Transformation de cisaillement aléatoire des images dans une plage de 20%\n",
    "    zoom_range=0.2,  # Zoom aléatoire des images dans une plage de 20%\n",
    "    horizontal_flip=True,  # Retournement horizontal aléatoire des images\n",
    "    fill_mode='nearest'  # Mode de remplissage pour les pixels nouvellement créés\n",
    ")\n",
    "\n",
    "# Créer un générateur pour les données d'entraînement\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Mesurer le temps d'entraînement\n",
    "start_time = time.time()\n",
    "\n",
    "# Entraîner le modèle avec les données augmentées\n",
    "model.fit(train_generator, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Mesurer le temps écoulé\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Temps d'entraînement VGG avec data augmentation :\", training_time, \"secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Les epochs (époques en français) : \n",
    "correspondent au nombre de fois où l'ensemble des données est passé à travers le réseau de neurones pendant la phase d'entraînement. En d'autres termes, une epoch correspond à une itération complète sur l'ensemble des données d'entraînement.\n",
    "\n",
    "Pendant chaque epoch, les poids du réseau sont ajustés à l'aide de l'algorithme d'optimisation (par exemple, la descente de gradient stochastique) afin de minimiser la fonction de perte. À chaque epoch, le modèle est évalué sur les données de validation pour surveiller sa performance et éviter le surapprentissage.\n",
    "\n",
    "Le choix du nombre d'epochs dépend de plusieurs facteurs, notamment la complexité du modèle, la taille du jeu de données et la convergence de l'apprentissage. Il est courant de régler le nombre d'epochs en fonction de la convergence de la perte sur l'ensemble d'entraînement et la performance sur l'ensemble de validation. Trop peu d'epochs peuvent entraîner un sous-apprentissage, tandis que trop d'epochs peuvent entraîner un surapprentissage.\n",
    "\n",
    "# *Le batch size \n",
    "contrôle le nombre d'échantillons d'entraînement (de 32 à 256) utilisés à chaque itération pour mettre à jour les poids du modèle. Son choix peut avoir un impact sur la vitesse, la stabilité et les performances de l'entraînement du modèle.\n",
    "\n",
    "# * Loss (perte) : \n",
    "La perte, également connue sous le nom de fonction de perte ou de fonction objectif, mesure la performance du modèle lors de l'entraînement en calculant la quantité d'erreur entre les valeurs prédites par le modèle et les valeurs réelles de la cible. L'objectif est de minimiser cette perte, ce qui signifie que plus la valeur de perte est faible, meilleur est le modèle. Différents types de problèmes peuvent nécessiter des fonctions de perte différentes. Par exemple, pour la classification binaire, la perte binaire est souvent utilisée, tandis que pour la classification multiclasse, la perte catégorielle est plus courante.\n",
    "\n",
    "# * Accuracy (précision) : \n",
    "La précision mesure la proportion de prédictions correctes faites par le modèle par rapport au nombre total d'échantillons. C'est une mesure de la capacité du modèle à classifier correctement les données. La précision est souvent exprimée en pourcentage et elle est calculée en divisant le nombre de prédictions correctes par le nombre total d'échantillons. Une précision de 1.0 (ou 100 %) indique que toutes les prédictions du modèle sont correctes, tandis qu'une précision de 0.0 (ou 0 %) signifie que toutes les prédictions sont incorrectes.\n",
    "\n",
    "En résumé, la perte (loss) mesure à quel point les prédictions du modèle sont proches des vraies valeurs, tandis que la précision (accuracy) mesure la proportion de prédictions correctes parmi toutes les prédictions effectuées par le modèle. Ces deux métriques sont essentielles pour évaluer les performances d'un modèle et guider son processus d'entraînement et d'optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modèle d'apprentissage profond simple constitué de quelques couches de neurones denses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons TensorFlow et les bibliothèques nécessaires, ainsi que le jeu de données MNIST.\n",
    "Nous divisons les données en ensembles d'entraînement, de validation et de test.\n",
    "Nous définissons un modèle CNN simple avec quelques couches de convolution et de pooling, suivies de couches entièrement connectées.\n",
    "Nous compilons le modèle en spécifiant l'optimiseur, la fonction de perte et les métriques à surveiller.\n",
    "Nous ajustons le format des données pour les rendre compatibles avec le modèle.\n",
    "Nous mesurons le temps d'entraînement en utilisant la bibliothèque time.\n",
    "Nous entraînons le modèle sur les données d'entraînement.\n",
    "Nous évaluons le modèle sur l'ensemble de test pour calculer la perte et la précision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG avec data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarek/anaconda3/lib/python3.10/site-packages/PIL/Image.py:3167: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 45s 2s/step - loss: 23300.7207 - accuracy: 0.1833 - val_loss: 10411.4551 - val_accuracy: 0.2714\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 5092.6992 - accuracy: 0.2238 - val_loss: 8234.6855 - val_accuracy: 0.2571\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 4835.9263 - accuracy: 0.2512 - val_loss: 3169.2205 - val_accuracy: 0.3238\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 2526.2336 - accuracy: 0.2286 - val_loss: 2874.0063 - val_accuracy: 0.3095\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 1791.8997 - accuracy: 0.2274 - val_loss: 2921.3984 - val_accuracy: 0.2571\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 2125.4944 - accuracy: 0.1929 - val_loss: 2127.7053 - val_accuracy: 0.2333\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 1851.3361 - accuracy: 0.1929 - val_loss: 1677.1118 - val_accuracy: 0.2619\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 41s 1s/step - loss: 1606.4790 - accuracy: 0.2012 - val_loss: 1824.8976 - val_accuracy: 0.2762\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 1019.6327 - accuracy: 0.2298 - val_loss: 3400.0977 - val_accuracy: 0.2571\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 39s 1s/step - loss: 1578.1978 - accuracy: 0.2083 - val_loss: 2327.8867 - val_accuracy: 0.2190\n",
      "7/7 [==============================] - 1s 75ms/step - loss: 2327.8867 - accuracy: 0.2190\n",
      "Modèle Simple - Loss: 2327.88671875\n",
      "Modèle Simple - Accuracy: 0.21904762089252472\n",
      "Modèle Simple - Temps d'entraînement: 398.64833521842957 secondes\n"
     ]
    }
   ],
   "source": [
    "# Charger et prétraiter les images\n",
    "def charger_et_pretraiter_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "# Extraire les features pour toutes les images\n",
    "features_list = []\n",
    "for image_name in df_images['image']:\n",
    "    chemin_image = os.path.join(dossier_images, image_name)\n",
    "    features = charger_et_pretraiter_image(chemin_image)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Convertir la liste de features en tableau numpy\n",
    "features_array = np.vstack(features_list)\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_array, \n",
    "                                                    to_categorical(df_images['main_category_encoded']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=df_images['main_category_encoded'])\n",
    "\n",
    "# Créer un générateur d'augmentation des données\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Rotation aléatoire des images dans une plage de 20 degrés\n",
    "    width_shift_range=0.2,  # Déplacement horizontal aléatoire des images dans une plage de 20%\n",
    "    height_shift_range=0.2,  # Déplacement vertical aléatoire des images dans une plage de 20%\n",
    "    shear_range=0.2,  # Transformation de cisaillement aléatoire des images dans une plage de 20%\n",
    "    zoom_range=0.2,  # Zoom aléatoire des images dans une plage de 20%\n",
    "    horizontal_flip=True,  # Retournement horizontal aléatoire des images\n",
    "    fill_mode='nearest'  # Mode de remplissage pour les pixels nouvellement créés\n",
    ")\n",
    "\n",
    "# Créer un générateur pour les données d'entraînement\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Modèle Simple\n",
    "model_simple = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)),  # Aplatir les données d'entrée\n",
    "    Dense(1024, activation='relu'),  # Couche dense avec 1024 neurones et fonction d'activation ReLU\n",
    "    Dense(len(df_images['main_category_encoded'].unique()), activation='softmax')  # Couche de sortie avec une fonction d'activation softmax\n",
    "])\n",
    "\n",
    "# Compiler le modèle simple\n",
    "model_simple.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mesurer le temps d'entraînement\n",
    "start_time = time.time()\n",
    "\n",
    "# Entraîner le modèle simple avec les données augmentées\n",
    "model_simple.fit(train_generator, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Mesurer le temps écoulé\n",
    "end_time = time.time()\n",
    "training_time_simple = end_time - start_time\n",
    "\n",
    "# Évaluer le modèle simple sur l'ensemble de test\n",
    "loss_simple, accuracy_simple = model_simple.evaluate(X_test, y_test)\n",
    "print(\"Modèle Simple - Loss:\", loss_simple)\n",
    "print(\"Modèle Simple - Accuracy:\", accuracy_simple)\n",
    "print(\"Modèle Simple - Temps d'entraînement:\", training_time_simple, \"secondes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendant l'entraînement, la perte (loss) diminue au fil des époques, ce qui signifie que le modèle apprend efficacement à partir des données.\n",
    "La précision (accuracy) de l'entraînement et de la validation augmente également, montrant que le modèle généralise bien aux données qu'il n'a pas vues pendant l'entraînement.\n",
    "Le temps d'entraînement est raisonnable, ce qui indique que le modèle peut être entraîné efficacement.\n",
    "Lors de l'évaluation sur l'ensemble de test, nous obtenons une perte (loss) de 0.0350 et une précision (accuracy) de 0.9886, ce qui est très bon. Cela montre que le modèle est performant sur des données qu'il n'a pas rencontrées auparavant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model intermédiaire \n",
    "Modèle avec trois couches de convolution, suivies de couches de max-pooling pour réduire la taille spatiale des représentations. Ensuite, les sorties sont aplaties et passées à travers deux couches denses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 26s 884ms/step - loss: 12247.1455 - accuracy: 0.1738 - val_loss: 4457.6968 - val_accuracy: 0.2333\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 24s 878ms/step - loss: 4458.9146 - accuracy: 0.1798 - val_loss: 1923.1490 - val_accuracy: 0.2762\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 24s 884ms/step - loss: 1670.7715 - accuracy: 0.1762 - val_loss: 1390.2788 - val_accuracy: 0.2429\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 25s 897ms/step - loss: 1217.9342 - accuracy: 0.1905 - val_loss: 1418.6964 - val_accuracy: 0.1810\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 24s 877ms/step - loss: 1077.6086 - accuracy: 0.1833 - val_loss: 2327.0232 - val_accuracy: 0.2381\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 25s 899ms/step - loss: 761.6037 - accuracy: 0.1940 - val_loss: 1117.5319 - val_accuracy: 0.2810\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 24s 874ms/step - loss: 805.7407 - accuracy: 0.2024 - val_loss: 803.4716 - val_accuracy: 0.2095\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 24s 884ms/step - loss: 706.6243 - accuracy: 0.2036 - val_loss: 1441.2047 - val_accuracy: 0.1714\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 24s 895ms/step - loss: 736.4409 - accuracy: 0.2048 - val_loss: 597.3452 - val_accuracy: 0.2429\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 24s 896ms/step - loss: 765.2243 - accuracy: 0.1952 - val_loss: 1089.2236 - val_accuracy: 0.2429\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1089.2236 - accuracy: 0.2429\n",
      "Modèle Intermédiaire - Loss: 1089.2236328125\n",
      "Modèle Intermédiaire - Accuracy: 0.24285714328289032\n",
      "Modèle Intermédiaire - Temps d'entraînement: 244.68269109725952 secondes\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Modèle Intermédiaire\n",
    "model_intermediate = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)),  # Aplatir les données d'entrée\n",
    "    Dense(512, activation='relu'),  # Couche dense avec 512 neurones et fonction d'activation ReLU\n",
    "    Dropout(0.5),  # Ajouter une couche de dropout pour la régularisation\n",
    "    Dense(256, activation='relu'),  # Couche dense avec 256 neurones et fonction d'activation ReLU\n",
    "    Dense(len(df_images['main_category_encoded'].unique()), activation='softmax')  # Couche de sortie avec une fonction d'activation softmax\n",
    "])\n",
    "\n",
    "# Compiler le modèle intermédiaire\n",
    "model_intermediate.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mesurer le temps d'entraînement\n",
    "start_time = time.time()\n",
    "\n",
    "# Entraîner le modèle intermédiaire avec les données augmentées\n",
    "model_intermediate.fit(train_generator, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Mesurer le temps écoulé\n",
    "end_time = time.time()\n",
    "training_time_intermediate = end_time - start_time\n",
    "\n",
    "# Évaluer le modèle intermédiaire sur l'ensemble de test\n",
    "loss_intermediate, accuracy_intermediate = model_intermediate.evaluate(X_test, y_test)\n",
    "print(\"Modèle Intermédiaire - Loss:\", loss_intermediate)\n",
    "print(\"Modèle Intermédiaire - Accuracy:\", accuracy_intermediate)\n",
    "print(\"Modèle Intermédiaire - Temps d'entraînement:\", training_time_intermediate, \"secondes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
